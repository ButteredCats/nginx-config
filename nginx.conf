# Run as the http user
user http;

# Headers more for all of our header stuff
# Its just easier than dealing with add_header deleting things sometimes
# It also allows easy overriding of set headers and clearing the server header
# https://github.com/openresty/headers-more-nginx-module
load_module /usr/lib/nginx/modules/ngx_http_headers_more_filter_module.so;

# Brotli compression
# https://github.com/google/ngx_brotli
load_module /usr/lib/nginx/modules/ngx_http_brotli_filter_module.so;

# Status page
# https://github.com/vozlt/nginx-module-vts
load_module /usr/lib/nginx/modules/ngx_http_vhost_traffic_status_module.so;

# Dynamic ETags to deal with proxies that have no built in caching ability
# https://github.com/dvershinin/ngx_dynamic_etag
load_module /usr/lib/nginx/modules/ngx_http_dynamic_etag_module.so;

# Enable PCRE JIT to speed up regex
# https://nginx.org/en/docs/ngx_core_module.html#pcre_jit
pcre_jit on;

# Automatically makes one worker process for each core
# https://nginx.org/en/docs/ngx_core_module.html#worker_processes
worker_processes auto;

# This controls how many open files each worker can have
# Each connection creates two files, one for up and one for down
# Therefore we want to set this high so we don't reject anyone
# Check the maximum possible value for the system by running "ulimit -Hn",
# this is the hard limit, and this value can be set anywhere below that
# 65535 was chosen as it's a common large value found online including in official posts
# https://nginx.org/en/docs/ngx_core_module.html#worker_rlimit_nofile
# https://www.f5.com/company/blog/nginx/using-nginx-plus-with-selinux
worker_rlimit_nofile 65535;

events {
	# How many connections each worker can handle
	# This cannot exceed worker_rlimit_nofile above
	# https://nginx.org/en/docs/ngx_core_module.html#worker_connections
	worker_connections 65535;
	
	# Reduce latency by accepting all new connections at once instead of one by one
	# Default is off and it's recommended to keep it that way, test with both
	# https://blog.nginx.org/blog/performance-tuning-tips-tricks
	# https://nginx.org/en/docs/ngx_core_module.html#multi_accept
	#multi_accept on;
}

http {
	### Default stuff ###
	include mime.types;


	### Disable logs ###
	access_log	off;
	error_log	/dev/null emerg;


	### HTTP versions ###
	# https://nginx.org/en/docs/http/ngx_http_v2_module.html#http2
	http2 on;

	# https://nginx.org/en/docs/http/ngx_http_v3_module.html#http3
	http3 on;


	### VTS module logging ###
	# Enable logging
	vhost_traffic_status_zone;

	# Split hosts in the same server block into different zones
	vhost_traffic_status_filter_by_host on;

	# Log data about individual HTTP versions
	vhost_traffic_status_filter_by_set_key $server_protocol $server_name;


	### Performance options ###
	# Skip copying files into a buffer
	# https://docs.nginx.com/nginx/admin-guide/web-server/serving-static-content/#enable-sendfile
	sendfile on;

	# Enables sending more data at once when combined with sendfile
	# https://nginx.org/en/docs/http/ngx_http_core_module.html#tcp_nopush
	tcp_nopush on;

	# Send a reset on closed or timed out connections, freeing up the memory being used on the socket
	# https://nginx.org/en/docs/http/ngx_http_core_module.html#reset_timedout_connection
	reset_timedout_connection on;

	# Consider aio in the future
	# https://nginx.org/en/docs/http/ngx_http_core_module.html#aio

	### Fixes ###
	# Makes NGINX continue to resolve hostnames after initial startup
	# This address is systemd-resolved's stub listener
	# https://nginx.org/en/docs/http/ngx_http_core_module.html#resolver
	resolver 127.0.0.53;
	
	# Increasing hash lengths
	# https://nginx.org/en/docs/hash.html

	# Increase types hash max size to avoid error
	# https://nginx.org/en/docs/http/ngx_http_core_module.html#types_hash_max_size
	types_hash_max_size 4096;

	# Allow longer server names
	# NGINX didn't prompt max size and bucket size, only bucket size
	# https://nginx.org/en/docs/http/ngx_http_core_module.html#server_names_hash_bucket_size
	server_names_hash_bucket_size 128;
	
	# Increase map hash bucket size to avoid error
	# NGINX didn't prompt max size and bucket size, only bucket size
	# https://nginx.org/en/docs/http/ngx_http_map_module.html#map_hash_bucket_size
	map_hash_bucket_size 128;

	# Make ratelimiting return 429 instead of 503 when a client is ratelimited
	# https://nginx.org/en/docs/http/ngx_http_limit_req_module.html#limit_req_status
	limit_req_status 429;


	### Hide version and name ###
	# The server name within error pages still remains, but this at least makes it harder for automated tools to just check the server header or hit an error page and get a version
	# https://nginx.org/en/docs/http/ngx_http_core_module.html#server_tokens
	server_tokens off;

	# Server is from NGINX, X-Powered-By is from PHP
	more_clear_headers Server X-Powered-By;


	### Uptime checker cache exemption ###
	# Define our uptime services by using a special string inside of a header
	include private/map-is-uptime-service.conf;
	
	# I don't want to keep things private, but it needed to be done to stop bad actors from easily bypassing the cache
	# The contents of the file look like the below, except the special string has been replaced:
	#	map $http_uptime_service $is_uptime_service {
	#		default				0;
	#		'special_string'	1;
	#	}

	# Make our uptime checkers bypass the cache for their checks and don't cache what they request
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache_bypass
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_no_cache
	proxy_cache_bypass $is_uptime_service;
	proxy_no_cache $is_uptime_service;


	### Compression ###
	# This was originally handled by the "gzip_vary" directive
	# In order to cleanly override it later when we need to vary on more than just compression we need to define it with more_set_headers
	# It only applies to the things we actually compress
	# Although we don't have a Cache-Control header for everything it still needs be sent because of heuristic caching
	more_set_headers 'Vary: Accept-Encoding' -t 'application/atom+xml application/javascript application/json application/vnd.api+json application/rss+xml
		application/vnd.ms-fontobject application/x-font-opentype application/x-font-truetype
		application/x-font-ttf application/x-javascript application/xhtml+xml application/xml
		font/eot font/opentype font/otf font/truetype image/svg+xml image/vnd.microsoft.icon
		image/x-icon image/x-win-bitmap text/css text/javascript text/plain text/xml text/html';

	# Brotli compression
	brotli on;
	# 0-11, anything above 9 begins to have huge response time impacts
	# Brotli requires more time to decompress than gzip or zstd at anything above 6
	# At 6 it takes basically the same amount of time while still producing smaller files
	brotli_comp_level 6;
	# With the test file brotli wasn't compressing well after this general range
	brotli_min_length 256;
	brotli_types application/atom+xml application/javascript application/json application/vnd.api+json application/rss+xml
		application/vnd.ms-fontobject application/x-font-opentype application/x-font-truetype
		application/x-font-ttf application/x-javascript application/xhtml+xml application/xml
		font/eot font/opentype font/otf font/truetype image/svg+xml image/vnd.microsoft.icon
		image/x-icon image/x-win-bitmap text/css text/javascript text/plain text/xml;

	# Gzip compression
	gzip on;
	# 1-9
	# Compression time spikes after 7
	gzip_comp_level 7;
	# With the test file gzip wasn't compressing well after this general range
	gzip_min_length 384;
	# Compress any types specified below, even if coming from a proxy
	# https://nginx.org/en/docs/http/ngx_http_gzip_module.html#gzip_proxied
	gzip_proxied any;
	gzip_types application/atom+xml application/javascript application/json application/vnd.api+json application/rss+xml
		application/vnd.ms-fontobject application/x-font-opentype application/x-font-truetype
		application/x-font-ttf application/x-javascript application/xhtml+xml application/xml
		font/eot font/opentype font/otf font/truetype image/svg+xml image/vnd.microsoft.icon
		image/x-icon image/x-win-bitmap text/css text/javascript text/plain text/xml;


	### Buffers ###
	# These are the "general" numbers, and any services that require other ones are set within their respective configurations
	# If anything exceeds the buffers it'll either have to be written to disk first, incurring extra latency, or can either return nothing to the client or a proper NGINX error page

	# This is the maximum amount of data a client can POST in a single request, any larger and we return HTTP 413
	# https://nginx.org/en/docs/http/ngx_http_core_module.html#client_max_body_size
	client_max_body_size 1k;

	# This is a buffer for POST requests
	# Worst case it gets a bit of a latency penalty from being written to disk, this one doesn't error out
	# Probably best to keep it in sync with client_max_body_size unless it's set extremely high
	# https://nginx.org/en/docs/http/ngx_http_core_module.html#client_body_buffer_size
	client_body_buffer_size 1k;

	# Typical client headers fit within this sized buffer
	# Used to be 1k but with Anubis having a cookie on every site this has been bumped to 2k
	# https://nginx.org/en/docs/http/ngx_http_core_module.html#client_header_buffer_size
	client_header_buffer_size 2k;

	# If a request from a client exceeds our client_header_buffer_size then this will try and fit the extra headers that couldn't fit
	# These are only allocated when needed meaning there isn't any downside besides more resource usage from a malicious client
	# Though the documentation reports either HTTP 400 or 414 should be returned when it's exceeded it doesn't seem to,
	# instead the client will recieve a secure connection error (curl reports "Error in the HTTP2 framing layer")
	# A 4k buffer was chosen because that's the max cookie size major browsers allow
	# https://nginx.org/en/docs/http/ngx_http_core_module.html#large_client_header_buffers
	large_client_header_buffers 1 4k;

	# This is the buffer size for the actual data sent by the proxy, not the headers
	# Equals 1m. Easily enough for text data and some small images
	# This isn't set anywhere else since all of our proxies are liable to send larger images that could fill this
	# This can be exceeded without any errors
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_buffers
	proxy_buffers 32 32k;

	# This is the buffer size for headers sent by the proxy
	# If what the proxy sends exceeds this we'll error out
	# Some applications have exceeded 1k in the past, so we use 2k for this
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_buffer_size
	proxy_buffer_size 2k;


	### Proxy options ###
	# If we can't even connect in 5 seconds the backend probably isn't working
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_connect_timeout
	proxy_connect_timeout 5s;

	# Host header is set because it's commonly used by the proxies and doesn't hurt to set
	# Connection '' enables keepalives
	# Accept-Encoding '' disables accepting any compression from the backend, it's only be a waste to compress for local transfers
	# also allows us to use our own encoding
	# If more need to be set anywhere use more_set_input_headers or it'll override these
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_set_header
	proxy_set_header Host "$host";
	proxy_set_header Connection '';
	proxy_set_header Accept-Encoding '';

	# Always upgrade proxy connection to HTTP/1.1 for keepalives
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_http_version
	proxy_http_version 1.1;

	# Swap to the next upstream in any case possible. max_fails=0 makes sure it'll just move onto the next upstream without marking it as down
	# Next request it'll try that upstream again and fall over again if needed
	# Worst case this just results in a longer response time on a normal request, best case it lets us serve a working page when one upstream is misbehaving
	# These also control what's considered a fail for max_tries and fail_timeout, with the exception that 403 and 404 will never count
	# We may need to change proxy_read_timeout to be lower to prevent hanging on one backend for a full minute
	# These are sane defaults though
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_next_upstream
	proxy_next_upstream error timeout invalid_header non_idempotent http_429 http_500 http_502 http_503 http_504;

	# Notes on proxy caching:
	# NGINX's cache should do the following
	# 1. Reduce hits to the backend for external items so we don't get ratelimited by external services
	# 2. Serve cached items when the backend is having issues to help uptime
	# 3. Reduce latency for external static items such as images by storing them locally
	# 4. It should accomplish the above without serving out of date items
	# ------
	# Reducing hits to the backend in order to reduce ratelimiting is typically done by caching HTML pages that frontends fetch.
	# This isn't always feasible though, take Redlib for example, it has a ton of cookies that change what a page could look like
	# and due to this proxy caching is disabled since it'd have an extremely low hit rate.
	# ------
	# The time saved by caching items local to the proxy like CSS, JS, or the favicon isn't worth potentially
	# serving an outdated version for a long period of time while serving up to date HTML pages that may depend on them.
	# Because of this the default valid cache time is kept low so that it basically only exists for the purpose of being served when the backend won't serve them.
	# ------
	# Static external items, like images, do have significant time saves from caching since we don't have to refetch them.
	# These will have a one year cache time set in that site's config so that we don't have to ever refetch them since they should never change.
	# They'll get removed from the cache if inactive for too long or if the cache is full and it's the least recently used item in the cache.
	# On top of that these items will have proxy_cache_lock enabled so that we only hit the resource once, further avoiding extra requests.
	# -----
	# Not serving out of date items can be hard.
	# As a general rule, we have a proxy cache valid time of one minute set below.
	# This should be safe for anything as it'll get revalidated extremely quickly while still being in the cache ready to be served when the backend breaks.
	# However, this can be made better for resources with version control, where they can be cached for longer because each new version will become a new cache element.
	# That way different versions can be requested by clients, for example if we serve them an old version of a page that had a different CSS version

	# Specify some common non-failure responses as valid for 1 minute, see notes above for reasoning
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache_valid
	proxy_cache_valid 200 204 206 301 302 304 307 308 1m;
	# Though it isn't specified here, by default only GET and HEAD requests will be cached

	# When updating expired items in the cache send If-Modified-Since so we don't have to receive the whole thing if it hasn't been updated
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache_revalidate
	# https://blog.nginx.org/blog/nginx-caching-guide
	proxy_cache_revalidate on;

	# Allow using stale cached items by when there's an error
	# Not including updating because we always want to serve the most up to date version
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache_use_stale
	proxy_cache_use_stale error timeout invalid_header http_404 http_429 http_500 http_502 http_503 http_504;

	# Ignore directives from proxy for caching and also don't allow clients to disable proxy buffering
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_cache_valid
	# https://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_buffering
	proxy_ignore_headers Cache-Control Expires X-Accel-Expires X-Accel-Buffering;


	### Anubis ###
	# Anubis needs global upstreams
	upstream anubis_staging {
		server unix:/run/anubis/staging/anubis.sock max_fails=0;
		keepalive 2;
	}
	upstream anubis_production {
		server unix:/run/anubis/production/anubis.sock max_fails=0;
		keepalive 2;
	}


	### Include our enabled sites ###
	include sites-enabled/*;
}
